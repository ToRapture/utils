#!/usr/bin/env python3

import argparse
import json
import logging
import os
import subprocess
import threading

import dateutil.parser
import twint


class Downloader:
    def __init__(self, username, output_dir, threads):
        self.username = username
        self.output_dir = output_dir
        self.threads = threads
        self.tweets = []

    def get_tweets(self):
        c = twint.Config(Username=self.username, Store_object=True, Hide_output=True)
        tweets_search = []
        c.Store_object_tweets_list = tweets_search
        twint.run.Search(c)
        logging.info('get %d tweets from search' % len(tweets_search))

        c = twint.Config(Username=self.username, Store_object=True, Hide_output=True)
        tweets_profile = []
        c.Store_object_tweets_list = tweets_profile
        twint.run.Profile(c)
        logging.info('get %d tweets from profile' % len(tweets_profile))

        ids_from_search = {t.id for t in tweets_search}
        tweets_merged = tweets_search + [t for t in tweets_profile if t.id not in ids_from_search]
        logging.info('get %d tweets after merged' % len(tweets_merged))

        self.tweets = tweets_merged

    def save_tweets(self):
        for t in self.tweets:
            self.save_one_tweet(t)

    def save_one_tweet(self, tweet):
        date = dateutil.parser.parse(tweet.datetime)
        tweet_path = os.path.join(self.output_dir, self.username, '%s %s' % (date.strftime('%Y-%m-%d %H-%M-%S %Z'), tweet.id))
        file_path = os.path.join(tweet_path, 'content.json')
        os.makedirs(tweet_path, exist_ok=True)
        with open(file_path, 'w') as f:
            json.dump(tweet.__dict__, f, indent=2, ensure_ascii=False)

    def save_medias(self):
        works = [[] for i in range(self.threads)]
        failed_links = [[] for i in range(self.threads)]
        for i, t in enumerate(self.tweets):
            works[i % self.threads].append(t)

        def _run_thread(tweets, failed):
            for t in tweets:
                returncode = self.save_one_media(t)
                if returncode:
                    failed.append(t.link)

        threads = []
        for i in range(self.threads):
            thread = threading.Thread(target=_run_thread, args=(works[i], failed_links[i]))
            threads.append(thread)
            thread.start()
        for thread in threads:
            thread.join()

        failed = sum(failed_links, [])
        logging.info('%d tweets save media failed: %s' % (len(failed), failed))

    def save_one_media(self, tweet):
        date = dateutil.parser.parse(tweet.datetime)
        tweet_path = os.path.join(self.output_dir, self.username, '%s %s' % (date.strftime('%Y-%m-%d %H-%M-%S %Z'), tweet.id))
        return subprocess.call(['you-get', tweet.link, '-o', tweet_path])

    def run(self):
        self.get_tweets()
        self.save_tweets()
        self.save_medias()


def init_log():
    logging.basicConfig(format='[%(levelname)s] %(asctime)s %(filename)s:%(lineno)s %(message)s', level=logging.INFO)


if __name__ == '__main__':
    init_log()

    parser = argparse.ArgumentParser(description='Download all tweets from Twitter.')
    parser.add_argument('--username', dest='username', required=True, action='store', type=str, help='username')
    parser.add_argument('--output', dest='output', default=os.getcwd(), action='store', type=str, help='output directory')
    parser.add_argument('--threads', dest='threads', default=8, action='store', type=int, help='number of threads')
    args = parser.parse_args()

    logging.info('argument: username is %s, output is %s, threads is %s', args.username, args.output, args.threads)

    Downloader(username=args.username, output_dir=args.output, threads=args.threads).run()
